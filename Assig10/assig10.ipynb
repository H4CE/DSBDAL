{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=\"\"\"I know you are an Army doctor, and you have been invalided home from Afghanistan. I know you have got a brother who is worried about you, but you won't go to him for help because you don not approve of him, possibly because he is an alcoholic, more likely because he recently walked out on his wife, and I know your therapist thinks your limp's psychosomatic - quite correctly, I'm afraid. That's enough to be going on with, don't you think? The name's Sherlock Holmes, and the address is 221-B Baker Street. Afternoon.\"\"\"\n",
    "#corpus means paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' I know you are an Army doctor, and you have been invalided home from Afghanistan.',\n",
       " \"I know you have got a brother who is worried about you, but you won't go to him for help because you don not approve of him, possibly because he is an alcoholic, more likely because he recently walked out on his wife, and I know your therapist thinks your limp's psychosomatic - quite correctly, I'm afraid.\",\n",
       " \"That's enough to be going on with, don't you think?\",\n",
       " \"The name's Sherlock Holmes, and the address is 221-B Baker Street.\",\n",
       " 'Afternoon.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenize\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "sentences=sent_tokenize(corpus)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'know',\n",
       " 'you',\n",
       " 'are',\n",
       " 'an',\n",
       " 'Army',\n",
       " 'doctor',\n",
       " ',',\n",
       " 'and',\n",
       " 'you',\n",
       " 'have',\n",
       " 'been',\n",
       " 'invalided',\n",
       " 'home',\n",
       " 'from',\n",
       " 'Afghanistan',\n",
       " '.',\n",
       " 'I',\n",
       " 'know',\n",
       " 'you',\n",
       " 'have',\n",
       " 'got',\n",
       " 'a',\n",
       " 'brother',\n",
       " 'who',\n",
       " 'is',\n",
       " 'worried',\n",
       " 'about',\n",
       " 'you',\n",
       " ',',\n",
       " 'but',\n",
       " 'you',\n",
       " 'wo',\n",
       " \"n't\",\n",
       " 'go',\n",
       " 'to',\n",
       " 'him',\n",
       " 'for',\n",
       " 'help',\n",
       " 'because',\n",
       " 'you',\n",
       " 'don',\n",
       " 'not',\n",
       " 'approve',\n",
       " 'of',\n",
       " 'him',\n",
       " ',',\n",
       " 'possibly',\n",
       " 'because',\n",
       " 'he',\n",
       " 'is',\n",
       " 'an',\n",
       " 'alcoholic',\n",
       " ',',\n",
       " 'more',\n",
       " 'likely',\n",
       " 'because',\n",
       " 'he',\n",
       " 'recently',\n",
       " 'walked',\n",
       " 'out',\n",
       " 'on',\n",
       " 'his',\n",
       " 'wife',\n",
       " ',',\n",
       " 'and',\n",
       " 'I',\n",
       " 'know',\n",
       " 'your',\n",
       " 'therapist',\n",
       " 'thinks',\n",
       " 'your',\n",
       " 'limp',\n",
       " \"'s\",\n",
       " 'psychosomatic',\n",
       " '-',\n",
       " 'quite',\n",
       " 'correctly',\n",
       " ',',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'afraid',\n",
       " '.',\n",
       " 'That',\n",
       " \"'s\",\n",
       " 'enough',\n",
       " 'to',\n",
       " 'be',\n",
       " 'going',\n",
       " 'on',\n",
       " 'with',\n",
       " ',',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'you',\n",
       " 'think',\n",
       " '?',\n",
       " 'The',\n",
       " 'name',\n",
       " \"'s\",\n",
       " 'Sherlock',\n",
       " 'Holmes',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'address',\n",
       " 'is',\n",
       " '221-B',\n",
       " 'Baker',\n",
       " 'Street',\n",
       " '.',\n",
       " 'Afternoon',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words=word_tokenize(corpus)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlp=spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I --> pronoun ( PRON )\n",
      "know --> verb ( VERB )\n",
      "you --> pronoun ( PRON )\n",
      "are --> auxiliary ( AUX )\n",
      "an --> determiner ( DET )\n",
      "Army --> proper noun ( PROPN )\n",
      "doctor --> noun ( NOUN )\n",
      ", --> punctuation ( PUNCT )\n",
      "and --> coordinating conjunction ( CCONJ )\n",
      "you --> pronoun ( PRON )\n",
      "have --> auxiliary ( AUX )\n",
      "been --> auxiliary ( AUX )\n",
      "invalided --> verb ( VERB )\n",
      "home --> adverb ( ADV )\n",
      "from --> adposition ( ADP )\n",
      "Afghanistan --> proper noun ( PROPN )\n",
      ". --> punctuation ( PUNCT )\n",
      "I --> pronoun ( PRON )\n",
      "know --> verb ( VERB )\n",
      "you --> pronoun ( PRON )\n",
      "have --> auxiliary ( AUX )\n",
      "got --> verb ( VERB )\n",
      "a --> determiner ( DET )\n",
      "brother --> noun ( NOUN )\n",
      "who --> pronoun ( PRON )\n",
      "is --> auxiliary ( AUX )\n",
      "worried --> adjective ( ADJ )\n",
      "about --> adposition ( ADP )\n",
      "you --> pronoun ( PRON )\n",
      ", --> punctuation ( PUNCT )\n",
      "but --> coordinating conjunction ( CCONJ )\n",
      "you --> pronoun ( PRON )\n",
      "wo --> auxiliary ( AUX )\n",
      "n't --> particle ( PART )\n",
      "go --> verb ( VERB )\n",
      "to --> adposition ( ADP )\n",
      "him --> pronoun ( PRON )\n",
      "for --> adposition ( ADP )\n",
      "help --> noun ( NOUN )\n",
      "because --> subordinating conjunction ( SCONJ )\n",
      "you --> pronoun ( PRON )\n",
      "don --> verb ( VERB )\n",
      "not --> particle ( PART )\n",
      "approve --> verb ( VERB )\n",
      "of --> adposition ( ADP )\n",
      "him --> pronoun ( PRON )\n",
      ", --> punctuation ( PUNCT )\n",
      "possibly --> adverb ( ADV )\n",
      "because --> subordinating conjunction ( SCONJ )\n",
      "he --> pronoun ( PRON )\n",
      "is --> auxiliary ( AUX )\n",
      "an --> determiner ( DET )\n",
      "alcoholic --> noun ( NOUN )\n",
      ", --> punctuation ( PUNCT )\n",
      "more --> adverb ( ADV )\n",
      "likely --> adjective ( ADJ )\n",
      "because --> subordinating conjunction ( SCONJ )\n",
      "he --> pronoun ( PRON )\n",
      "recently --> adverb ( ADV )\n",
      "walked --> verb ( VERB )\n",
      "out --> adposition ( ADP )\n",
      "on --> adposition ( ADP )\n",
      "his --> pronoun ( PRON )\n",
      "wife --> noun ( NOUN )\n",
      ", --> punctuation ( PUNCT )\n",
      "and --> coordinating conjunction ( CCONJ )\n",
      "I --> pronoun ( PRON )\n",
      "know --> verb ( VERB )\n",
      "your --> pronoun ( PRON )\n",
      "therapist --> noun ( NOUN )\n",
      "thinks --> verb ( VERB )\n",
      "your --> pronoun ( PRON )\n",
      "limp --> noun ( NOUN )\n",
      "'s --> particle ( PART )\n",
      "psychosomatic --> adjective ( ADJ )\n",
      "- --> punctuation ( PUNCT )\n",
      "quite --> adverb ( ADV )\n",
      "correctly --> adverb ( ADV )\n",
      ", --> punctuation ( PUNCT )\n",
      "I --> pronoun ( PRON )\n",
      "'m --> auxiliary ( AUX )\n",
      "afraid --> adjective ( ADJ )\n",
      ". --> punctuation ( PUNCT )\n",
      "That --> pronoun ( PRON )\n",
      "'s --> auxiliary ( AUX )\n",
      "enough --> adjective ( ADJ )\n",
      "to --> particle ( PART )\n",
      "be --> auxiliary ( AUX )\n",
      "going --> verb ( VERB )\n",
      "on --> adposition ( ADP )\n",
      "with --> adposition ( ADP )\n",
      ", --> punctuation ( PUNCT )\n",
      "do --> auxiliary ( AUX )\n",
      "n't --> particle ( PART )\n",
      "you --> pronoun ( PRON )\n",
      "think --> verb ( VERB )\n",
      "? --> punctuation ( PUNCT )\n",
      "The --> determiner ( DET )\n",
      "name --> noun ( NOUN )\n",
      "'s --> particle ( PART )\n",
      "Sherlock --> proper noun ( PROPN )\n",
      "Holmes --> proper noun ( PROPN )\n",
      ", --> punctuation ( PUNCT )\n",
      "and --> coordinating conjunction ( CCONJ )\n",
      "the --> determiner ( DET )\n",
      "address --> noun ( NOUN )\n",
      "is --> auxiliary ( AUX )\n",
      "221 --> numeral ( NUM )\n",
      "- --> punctuation ( PUNCT )\n",
      "B --> proper noun ( PROPN )\n",
      "Baker --> proper noun ( PROPN )\n",
      "Street --> proper noun ( PROPN )\n",
      ". --> punctuation ( PUNCT )\n",
      "Afternoon --> noun ( NOUN )\n",
      ". --> punctuation ( PUNCT )\n"
     ]
    }
   ],
   "source": [
    "#POS Tagging \n",
    "paragraph=nlp(corpus)\n",
    "\n",
    "for tokens in paragraph:\n",
    "    print(tokens,\"-->\",spacy.explain(tokens.pos_),\"(\",tokens.pos_,\")\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'various', 'become', 'others', 'what', 'can', 'two', 'further', 'those', 'â€™d', 'of', 'thereby', 'get', 'yourself', 'even', 'your', 'next', 'whether', 'no', 'been', 'via', 'except', 'ca', 'against', 'meanwhile', \"'m\", 'this', 'must', 'thence', 'over', 'until', 'to', 'some', 'someone', 'six', 'became', 'all', 'just', 'upon', 'its', 'regarding', 'anywhere', 'hence', 'several', 'whence', 'less', 'towards', 'therein', 'always', 'although', 'using', 'has', 'after', 'â€˜d', 'could', 'you', \"'d\", 'whereas', 'either', 'therefore', 'than', 'as', 'seeming', 'each', 'call', 'did', 'three', 'in', 'themselves', 'since', 'am', 'anything', 'latter', 'last', 'â€˜ll', 'once', 'have', 'fifty', 'sometimes', 'by', 'them', 'â€™re', 'almost', 'together', 'moreover', 'cannot', 'twenty', 'wherever', 'but', 'be', 'between', 'were', 'here', 'both', 'will', 'may', 'never', 'most', 'perhaps', 'seems', 'nor', 'everywhere', 'among', 'during', 'my', 'are', 'fifteen', 'â€™s', 'whenever', 'per', 'beyond', 'twelve', 'he', 'forty', 'myself', 'his', 'thereupon', 'still', 'unless', 'our', 'sometime', 'me', 'name', 'done', 'wherein', 'anyhow', 'then', 'give', 'also', 'should', 'nine', 'thru', 'whatever', 'five', 'â€™ve', 'other', 'else', 'onto', 'the', 'that', 'really', 'four', 'many', 'somewhere', 'due', 'very', 'toward', 'latterly', 'somehow', 'keep', 'take', 'well', 'or', 'might', 'herein', 'whose', 'such', 'they', 'mine', 'now', 'too', 'so', 'go', 'with', \"'ve\", 'alone', 'out', 'besides', 'from', 'own', 'without', 'former', 'why', 'throughout', 'â€˜m', 'anyway', 'an', 'put', 'down', 'move', 'least', 'every', 'amongst', 'their', 'is', \"'ll\", 'nobody', 'itself', 'â€™m', 'where', 'full', 'few', 'ten', 'under', 'side', 'front', 'afterwards', 'whereby', 'however', 'who', 'amount', 'becoming', 'not', 'see', 'make', 'first', 'below', 'her', 'sixty', 'for', 'empty', 'i', 'otherwise', 'same', 'more', 'which', \"'re\", 'before', 'seem', 'much', 'ours', 'nâ€™t', 'yours', 'please', 'beside', 'on', 'quite', 'â€™ll', 'any', 'himself', 'whoever', 'whole', 'around', 'bottom', 'him', 'only', 'â€˜s', 'again', 'across', 'used', 'nothing', 'there', 'show', 'everything', 'nevertheless', 'was', 'though', 'us', 'formerly', 'above', 'she', 'yet', 'if', 'â€˜ve', 'everyone', 'indeed', 'hereby', 'â€˜re', \"n't\", 'seemed', 'neither', 'none', 'thus', 'becomes', 're', 'while', 'and', 'something', 'beforehand', 'doing', 'into', 'nowhere', 'eleven', 'ourselves', 'third', 'being', 'often', 'hereupon', 'already', 'elsewhere', 'enough', 'top', 'whom', 'nâ€˜t', 'how', 'off', 'herself', 'another', 'hers', \"'s\", 'we', 'does', 'back', 'had', 'eight', 'about', 'part', 'hereafter', 'noone', 'say', 'it', 'whereafter', 'hundred', 'thereafter', 'ever', 'because', 'these', 'through', 'one', 'within', 'whereupon', 'anyone', 'yourselves', 'mostly', 'namely', 'a', 'whither', 'would', 'at', 'rather', 'when', 'made', 'up', 'along', 'serious', 'do', 'behind'}\n"
     ]
    }
   ],
   "source": [
    "#stop words removal\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[know,\n",
       " Army,\n",
       " doctor,\n",
       " invalided,\n",
       " home,\n",
       " Afghanistan,\n",
       " know,\n",
       " got,\n",
       " brother,\n",
       " worried,\n",
       " wo,\n",
       " help,\n",
       " don,\n",
       " approve,\n",
       " possibly,\n",
       " alcoholic,\n",
       " likely,\n",
       " recently,\n",
       " walked,\n",
       " wife,\n",
       " know,\n",
       " therapist,\n",
       " thinks,\n",
       " limp,\n",
       " psychosomatic,\n",
       " correctly,\n",
       " afraid,\n",
       " going,\n",
       " think,\n",
       " Sherlock,\n",
       " Holmes,\n",
       " address,\n",
       " 221,\n",
       " B,\n",
       " Baker,\n",
       " Street,\n",
       " Afternoon]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swRemoved=[]\n",
    "for word in paragraph:\n",
    "    if not word.is_stop and not word.is_punct:\n",
    "        swRemoved.append(word)\n",
    "swRemoved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(swRemoved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming and Lemmatization\n",
    "# stemming is applying brute force and removing the suffix from the word to get the original word\n",
    "# eating--> eat is stemming\n",
    "# but stemming can not do ate-->eat. For this a knowledge of the language is required which is done in lemmatization\n",
    "\n",
    "#stemming not available in spacy so use nltk stemming\n",
    "from nltk.stem import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "know --> know\n",
      "Army --> armi\n",
      "doctor --> doctor\n",
      "invalided --> invalid\n",
      "home --> home\n",
      "Afghanistan --> afghanistan\n",
      "know --> know\n",
      "got --> got\n",
      "brother --> brother\n",
      "worried --> worri\n",
      "wo --> wo\n",
      "help --> help\n",
      "don --> don\n",
      "approve --> approv\n",
      "possibly --> possibl\n",
      "alcoholic --> alcohol\n",
      "likely --> like\n",
      "recently --> recent\n",
      "walked --> walk\n",
      "wife --> wife\n",
      "know --> know\n",
      "therapist --> therapist\n",
      "thinks --> think\n",
      "limp --> limp\n",
      "psychosomatic --> psychosomat\n",
      "correctly --> correctli\n",
      "afraid --> afraid\n",
      "going --> go\n",
      "think --> think\n",
      "Sherlock --> sherlock\n",
      "Holmes --> holm\n",
      "address --> address\n",
      "221 --> 221\n",
      "B --> b\n",
      "Baker --> baker\n",
      "Street --> street\n",
      "Afternoon --> afternoon\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "for word in swRemoved:\n",
    "    stemmed_word = stemmer.stem(word.text)\n",
    "    print(f\"{word} --> {stemmed_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "know --> know\n",
      "Army --> Army\n",
      "doctor --> doctor\n",
      "invalided --> invalid\n",
      "home --> home\n",
      "Afghanistan --> Afghanistan\n",
      "know --> know\n",
      "got --> get\n",
      "brother --> brother\n",
      "worried --> worried\n",
      "wo --> will\n",
      "help --> help\n",
      "don --> don\n",
      "approve --> approve\n",
      "possibly --> possibly\n",
      "alcoholic --> alcoholic\n",
      "likely --> likely\n",
      "recently --> recently\n",
      "walked --> walk\n",
      "wife --> wife\n",
      "know --> know\n",
      "therapist --> therapist\n",
      "thinks --> think\n",
      "limp --> limp\n",
      "psychosomatic --> psychosomatic\n",
      "correctly --> correctly\n",
      "afraid --> afraid\n",
      "going --> go\n",
      "think --> think\n",
      "Sherlock --> Sherlock\n",
      "Holmes --> Holmes\n",
      "address --> address\n",
      "221 --> 221\n",
      "B --> B\n",
      "Baker --> Baker\n",
      "Street --> Street\n",
      "Afternoon --> afternoon\n"
     ]
    }
   ],
   "source": [
    "for word in swRemoved:\n",
    "    print(word,\"-->\",word.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
